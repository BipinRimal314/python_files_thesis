Project Context: Unsupervised Behavioural Profiling for Insider Threat Detection1. Project MetadataTitle: Unsupervised Behavioural Profiling for Insider Threat Detection Using Time-Series and Anomaly Detection TechniquesAuthor: Bipin RimalDomain: Cybersecurity / Data Science / Computational IntelligenceCore Task: Detecting insider threats in enterprise log data using unsupervised machine learning.2. Executive Summary (Abstract)This project implements a data-centric, semi-supervised machine learning framework to detect insider threats. Unlike signature-based systems (which look for known malware), this system builds dynamic "profiles of normalcy" for users to detect anomalous behavior.Data Source: CMU-CERT dataset (Subsets r1, r2, r3.1), processing >38 million log events.Models Compared:Isolation Forest: Statistical/Ensemble method.Deep Clustering: K-Means Autoencoder (Static Deep Learning).LSTM Autoencoder: Sequential Time-Series (Deep Learning).Key Finding: The LSTM Autoencoder was the dominant driver of detection (Optimal Weight: 0.8), proving that sequential temporal reconstruction is superior to static volume counts for detecting insider malice.Performance: Achieved 66% Recall (2/3 insiders detected) with a manageable false positive rate in a strictly aligned test set.3. Problem StatementThe "Insider" Problem: Traditional security tools (Firewalls, IDS, SIEM) are designed for external threats. They fail against insiders because insiders have legitimate access. Malicious acts often look syntactically identical to normal work (e.g., a sysadmin copying files).The "Context Gap": Legacy systems are context-blind. They see "Access Granted" but don't understand if the access volume or timing is weird for that specific user.The Solution: Shift from Deductive (Rules/Signatures) to Inductive (Behavioral Profiling/UEBA). The system must learn what is "normal" and flag deviations.4. Methodology & Architecture4.1. Data StrategyDataset: CMU-CERT Insider Threat Dataset (Synthetic but realistic).Ingestion Strategy: Full-Volume Ingestion. The research rejected random sampling (e.g., 10%) because threat signals are so sparse (<0.01%) that sampling statistically destroys the signal ("Catastrophic Ingestion Bias").Processing: Implemented a multi-pass, streaming ETL pipeline in Python to handle 180GB+ of data on constrained hardware (24GB RAM).4.2. Feature Engineering (The "Contextual Z-Score")Raw counts (e.g., "50 emails") are insufficient. The project engineered Relative Entropy features:Self-Relative Z-Score ($Z_{self}$): How much does today's behavior deviate from this user's history? (Detects Bursts).Peer-Relative Z-Score ($Z_{peer}$): How much does this user deviate from their functional role (e.g., Sales vs. IT)? (Detects "Weird-but-Normal" outliers).4.3. Model ArchitecturesIsolation Forest: Detects point anomalies based on path length in random trees. Good for "few and different" outliers.Deep Clustering (Autoencoder + K-Means): Uses an autoencoder to compress data into a latent space, then clusters it. Failed in this experiment (Score 0.0 contribution).LSTM Autoencoder: A time-series model that learns the sequence of user actions. It is trained to reconstruct normal sequences; high "Reconstruction Error" indicates an anomaly. This was the most successful model.4.4. Experimental DesignTrain-on-Normal: Models are trained only on benign users to learn the manifold of normalcy.Test-on-Mixed: Models are evaluated on a mixed dataset containing both benign users and known insiders.5. Key Findings5.1. Sequential DominanceHypothesis Overturned: The initial fear was that "Sliding Windows" would dilute threat signals.Reality: The LSTM Autoencoder contributed 80% of the decision power in the final ensemble. Static models (Isolation Forest, Deep Clustering) contributed only 10% each. This proves that malice is a sequence disruption, not just a volume spike.5.2. The "Stealth Gap" (Ontological Ceiling)The system detected 2 out of 3 insiders (User 616, User 724).User 3908 (Undetected): This user remained mathematically invisible (Score 0.0). They likely used "Living-off-the-Land" techniquesâ€”malicious intent hidden within perfectly normal volume and tools. This represents the limit of metadata-only analysis.5.3. The Accuracy ParadoxIn a dataset where 99.9% of users are benign, a "Null Classifier" (predicting nothing) achieves ~99.9% accuracy.Therefore, Accuracy is a deceptive metric for cybersecurity. The project prioritized Recall (catching the bad guy) and Precision (minimizing false alerts).6. Critical Definitions for the AgentIngestion Bias: The error introduced by down-sampling data in cybersecurity; you cannot sample a dataset looking for a "needle in a haystack" without risking throwing away the needle.Base Rate Fallacy: The statistical reality that even a highly accurate test will generate massive false positives if the condition (malice) is extremely rare.Living-off-the-Land (LotL): Attackers using pre-installed, legitimate system tools (like PowerShell) to conduct attacks, making them invisible to signature detection.Sliding Window Fallacy: The incorrect assumption that breaking a timeline into fixed chunks (windows) destroys the semantic meaning of the events.7. Pipeline Artifactsdata_preprocessing_v2.py: Streaming ETL pipeline.feature_engineering_v2.py: Generates Z-scores and Sequences.lstm_model_v2.py: The core deep learning engine.diagnose_results_v2.py: Forensic evaluation script.