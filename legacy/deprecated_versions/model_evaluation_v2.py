"""
Comprehensive Model Evaluation (V2: Multi-Dataset)

This script loads the individual metric CSVs generated by each V2 model
(IF, DC, LSTM) and creates a unified evaluation report,
comparison table, and plots.

[--- V4.2 FINAL FIX ---]
- Handles missing metrics/prediction CSVs gracefully.
- Skips ROC/PR plots when y_true is not strictly binary.
- Fixes `ValueError: multiclass format...` safely.
- Improves console feedback for skipped models.
"""

import pandas as pd
import numpy as np
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import roc_curve, precision_recall_curve, auc
from sklearn.utils.multiclass import type_of_target

# Use the V2 config
import config_v2 as config
import utils

logger = utils.logger

class ModelEvaluatorV2:
    
    def __init__(self):
        self.results_dir = config.RESULTS_DIR
        self.viz_dir = config.RESULT_PATHS['visualizations']
        self.roc_dir = config.RESULT_PATHS['roc_curves']
        self.cm_dir = config.RESULT_PATHS['confusion_matrices']
        
        subset = config.DATASET_SUBSET if hasattr(config, 'DATASET_SUBSET') and config.DATASET_SUBSET else []
        self.subset_name = "_".join(subset) if subset else "ALL"
        
        self.viz_dir.mkdir(parents=True, exist_ok=True)
        self.roc_dir.mkdir(parents=True, exist_ok=True)
        self.cm_dir.mkdir(parents=True, exist_ok=True)
        
        plt.style.use(config.VISUALIZATION['style'])
        self.model_data = {}  # To store loaded data {model_name: {metrics: df, preds: df}}

    def load_all_results(self):
        logger.info("Loading V2 results for all models...")
        model_names = ['isolation_forest', 'deep_clustering', 'lstm_autoencoder']
        
        for name in model_names:
            metrics_path = self.results_dir / f"{name}_metrics_{self.subset_name}_v2.csv"
            preds_path = self.results_dir / f"{name}_predictions_{self.subset_name}_v2.csv"
            
            if not metrics_path.exists() or not preds_path.exists():
                logger.warning(f"Skipping {name}: missing results or predictions.")
                continue
            
            try:
                metrics_df = pd.read_csv(metrics_path)
                preds_df = pd.read_csv(preds_path)
                self.model_data[name] = {
                    'metrics': metrics_df.iloc[0].to_dict(),
                    'preds': preds_df
                }
                logger.info(f"Loaded results for {name}")
            except Exception as e:
                logger.error(f"Failed to load V2 results for {name}: {e}")
        
        if not self.model_data:
            raise FileNotFoundError("No V2 model results found. Please run the V2 training scripts first.")
        
        logger.info(f"Loaded results for {len(self.model_data)} models")

    def create_metrics_table(self) -> pd.DataFrame:
        logger.info("Creating metrics comparison table...")
        metrics_list = []
        for model_name, data in self.model_data.items():
            metrics = data['metrics']
            metrics['model'] = model_name
            metrics_list.append(metrics)
        
        metrics_df = pd.DataFrame(metrics_list).set_index('model')
        cols_to_show = ['auc_roc', 'f1_score', 'precision', 'recall', 'accuracy']
        available_cols = [col for col in cols_to_show if col in metrics_df.columns]
        metrics_df = metrics_df[available_cols + [c for c in metrics_df.columns if c not in available_cols]]
        
        logger.info("Metrics comparison table created")
        return metrics_df

    def plot_confusion_matrices(self):
        logger.info("Creating confusion matrix plots...")
        num_models = len(self.model_data)
        fig, axes = plt.subplots(1, num_models, figsize=(6 * num_models, 5))
        if num_models == 1:
            axes = [axes]

        for ax, (model_name, data) in zip(axes, self.model_data.items()):
            preds_df = data['preds']
            y_true = preds_df['true_label'].astype(int)
            y_pred = preds_df['prediction'].astype(int)
            cm = pd.crosstab(y_true, y_pred, rownames=['Actual'], colnames=['Predicted'])
            cm = cm.reindex(index=[0, 1], columns=[0, 1], fill_value=0)
            tn, fp, fn, tp = cm.loc[0, 0], cm.loc[0, 1], cm.loc[1, 0], cm.loc[1, 1]
            cm_annot = [[f"TN\n{tn}", f"FP\n{fp}"], [f"FN\n{fn}", f"TP\n{tp}"]]
            sns.heatmap(cm, annot=cm_annot, fmt='', cmap='Blues', ax=ax, cbar=False)
            ax.set_title(f"{model_name.replace('_', ' ').title()}")
        
        plt.tight_layout()
        save_path = self.cm_dir / f"all_models_{self.subset_name}_v2.png"
        plt.savefig(save_path, dpi=config.VISUALIZATION['dpi'])
        logger.info(f"Confusion matrices saved to {save_path}")
        plt.close()

    def plot_roc_curves(self):
        logger.info("Creating ROC curves...")
        plt.figure()
        skipped = 0
        
        for model_name, data in self.model_data.items():
            preds_df = data['preds']
            y_true = preds_df['true_label'].astype(int)
            y_score = preds_df['anomaly_score']

            # Skip if not binary
            if type_of_target(y_true) != 'binary':
                logger.warning(f"Skipping ROC for {model_name}: Non-binary or invalid label format ({type_of_target(y_true)})")
                skipped += 1
                continue
            
            if y_true.sum() == 0:
                logger.warning(f"Skipping ROC for {model_name}: No positive true labels.")
                skipped += 1
                continue

            fpr, tpr, _ = roc_curve(y_true, y_score)
            roc_auc = auc(fpr, tpr)
            plt.plot(fpr, tpr, label=f"{model_name.replace('_', ' ').title()} (AUC = {roc_auc:.4f})")
        
        if skipped < len(self.model_data):
            plt.plot([0, 1], [0, 1], 'k--', label='Chance (AUC = 0.50)')
            plt.xlim([0.0, 1.0])
            plt.ylim([0.0, 1.05])
            plt.xlabel('False Positive Rate')
            plt.ylabel('True Positive Rate')
            plt.title('Receiver Operating Characteristic (ROC) - V2 Models')
            plt.legend(loc="lower right")
            
            save_path = self.roc_dir / f"roc_comparison_{self.subset_name}_v2.png"
            plt.savefig(save_path, dpi=config.VISUALIZATION['dpi'])
            logger.info(f"ROC curves saved to {save_path}")
        else:
            logger.warning("All ROC plots skipped: non-binary label issues.")
        plt.close()
        
    def plot_precision_recall_curves(self):
        logger.info("Creating Precision-Recall curves...")
        plt.figure()
        skipped = 0
        
        for model_name, data in self.model_data.items():
            preds_df = data['preds']
            y_true = preds_df['true_label'].astype(int)
            y_score = preds_df['anomaly_score']

            if type_of_target(y_true) != 'binary':
                logger.warning(f"Skipping PR for {model_name}: Non-binary labels.")
                skipped += 1
                continue

            if y_true.sum() == 0:
                logger.warning(f"Skipping PR for {model_name}: No positive true labels.")
                skipped += 1
                continue

            precision, recall, _ = precision_recall_curve(y_true, y_score)
            pr_auc = auc(recall, precision)
            plt.plot(recall, precision, label=f"{model_name.replace('_', ' ').title()} (AP = {pr_auc:.4f})")

        if skipped < len(self.model_data):
            plt.xlabel('Recall')
            plt.ylabel('Precision')
            plt.title('Precision-Recall Curve - V2 Models')
            plt.legend(loc="lower left")
            save_path = self.viz_dir / f"precision_recall_comparison_{self.subset_name}_v2.png"
            plt.savefig(save_path, dpi=config.VISUALIZATION['dpi'])
            logger.info(f"Precision-Recall curves saved to {save_path}")
        else:
            logger.warning("All PR plots skipped: label issues.")
        plt.close()

    def generate_report(self, metrics_df: pd.DataFrame):
        logger.info("Generating evaluation report...")
        report_path = self.results_dir / f"evaluation_report_{self.subset_name}_v2.txt"
        
        report_header = f"""
================================================================================
                MODEL EVALUATION REPORT (V2: {self.subset_name})
================================================================================

Generated: {pd.Timestamp.now()}

This report summarizes the performance of all V2 models trained on the
'{self.subset_name}' dataset subset using advanced Z-score features.

================================================================================
PERFORMANCE METRICS COMPARISON (from default threshold)
================================================================================
{metrics_df.to_markdown(floatfmt=".4f")}
"""
        report_body = "\n"
        best_f1 = -1
        best_model = "N/A"

        for model_name, data in self.model_data.items():
            metrics = data['metrics']
            report_body += f"""
--------------------------------------------------------------------------------
{model_name.replace('_', ' ').title().upper()} - DETAILED METRICS
--------------------------------------------------------------------------------
"""
            for key, value in metrics.items():
                if key != 'model':
                    if isinstance(value, (int, float, np.number)):
                        report_body += f"{key.replace('_', ' ').title():<28}: {value:.4f}\n"
                    else:
                        report_body += f"{key.replace('_', ' ').title():<28}: {value}\n"
            current_f1 = metrics.get('f1_score', 0.0)
            if isinstance(current_f1, (int, float, np.number)) and current_f1 > best_f1:
                best_f1 = current_f1
                best_model = model_name.replace('_', ' ').title()

        f1_score_str = f"{best_f1:.4f}" if isinstance(best_f1, (int, float)) else str(best_f1)
        report_footer = f"""
================================================================================
BEST PERFORMING MODEL (by F1-Score)
================================================================================
Model: {best_model}
F1-Score: {f1_score_str}
================================================================================
"""
        final_report = report_header + report_body + report_footer
        with open(report_path, 'w') as f:
            f.write(final_report)
        print(final_report)
        logger.info(f"Evaluation report saved to {report_path}")

    def run_full_evaluation(self):
        logger.info(utils.generate_report_header("COMPREHENSIVE MODEL EVALUATION (V2)"))
        try:
            self.load_all_results()
            metrics_df = self.create_metrics_table()
            self.plot_confusion_matrices()
            self.plot_roc_curves()
            self.plot_precision_recall_curves()
            self.generate_report(metrics_df)
            logger.info("Comprehensive V2 evaluation completed successfully!")
        except Exception as e:
            logger.error(f"Evaluation failed: {e}")
            import traceback; traceback.print_exc()

def main():
    evaluator = ModelEvaluatorV2()
    evaluator.run_full_evaluation()

if __name__ == "__main__":
    main()
